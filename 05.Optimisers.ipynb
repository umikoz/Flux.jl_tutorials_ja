{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisers\n",
    "\n",
    "単純な線形回帰を考えよう。いくつかのダミーデータを作り、損失を計算し、パラメータ`W`と`b`のために勾配を計算するためにバックプロパゲーションする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l = loss(x, y) = 3.387532384382774 (tracked)\n",
      "Θ = Params([W, b]) = Params([Flux.Tracker.TrackedReal{Float64}[0.387905 (tracked), 0.721443 (tracked)], Flux.Tracker.TrackedReal{Float64}[0.905276 (tracked) 0.274859 (tracked) 0.425754 (tracked) 0.0999047 (tracked) 0.0495313 (tracked); 0.532982 (tracked) 0.111951 (tracked) 0.355829 (tracked) 0.486555 (tracked) 0.0532328 (tracked)]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Grads(...)\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "using Flux: Tracker, Params\n",
    "\n",
    "W = param(rand(2,5))\n",
    "b = param(rand(2))\n",
    "\n",
    "predict(x) = W*x .+b\n",
    "loss(x,y) = sum((predict(x) .-y).^2)\n",
    "\n",
    "x,y = rand(5), rand(2)\n",
    "@show l = loss(x,y) # ~3\n",
    "\n",
    "@show Θ　= Params([W,b])\n",
    "grads = Tracker.gradient(()->loss(x,y), Θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "損失を改善（削減）するため、勾配を使って、各パラメータを更新したい。  \n",
    "これをするための一つの方法は次の通り。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sgd (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux.Tracker: grad, update!\n",
    "\n",
    "function sgd()\n",
    "    η = 0.1　# learning rate\n",
    "    for p in (W,b)\n",
    "        update!(p,-η*grads[0])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sgd`を呼ぶなら、パラメータ`W`と`b`は変化して損失は下がるはず。\n",
    "\n",
    "ここには２つのピースがある。一つはモデルのための学習可能なパラメータのリスト（この場合は`[W, b]`）が必要なこと、そしてもう一つは更新ステップだ。  \n",
    "この場合、更新はシンプルに勾配降下法（`x .= η .* Δ`）だが、モーメンタム（運動量）を追加するなどのもっと高度なことを選択するかもしれない。\n",
    "\n",
    "この場合、編集を得るのは簡単だが、複雑なレイヤースタックがあると、より面倒になることが想像できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(10, 5, NNlib.σ), Dense(5, 2), NNlib.softmax)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Chain(\n",
    "    Dense(10,5,σ),\n",
    "    Dense(5,2),\n",
    "    softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[m[1].W, m[1].b, ...]`と書く代わりに、Fluxは君のために`params(m)`でモデルの中のすべてのパラメータのリストを返すparams関数を提供する。\n",
    "\n",
    "アップデートステップでは、上述のループを書いても心配は無い。これはうまくいく。  \n",
    "しかしFluxはそれをより便利にする様々なオプティマイザを提供する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD([W,b], 0.1) # 勾配降下法と学習率\n",
    "\n",
    "opt() # アップデートを実行して、Wとbを修正する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "オプティマイザはパラメータリストを取り、上述の`update`と同じことをする関数を返す。  \n",
    "`opt`か`update`をトレーニングループに渡すことができ、データの各ミニバッチの後にオプティマイザを実行するだろう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! paramsを使った場合はこう書けそう...\n",
    "opt2 = SGD(params(m), 0.1)\n",
    "opt2() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimiser Reference\n",
    "---\n",
    "すべてのOptimisersは、呼ばれたときに、渡されたパラメータを更新する関数を返す。\n",
    "\n",
    "[WIP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Building Basics\n",
    "\n",
    "## Taking Gradients\n",
    "\n",
    "Fluxのコア機能はJuliaコードの勾配をとっている。この`gradient`関数は別のjuliaの関数`f`と一連の引数を撮って、それぞれの引数に関する勾配を返す。（JuliaTerminalでこれらの例を貼り付けてみることを薦める。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df(2) = 14.0 (tracked)\n",
      "d2f(2) = 6.0 (tracked)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6.0 (tracked)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux.Tracker \n",
    "\n",
    "f(x) = 3x^2 + 2x + 1\n",
    "\n",
    "# df/dx = 6x + 2\n",
    "df(x) = Tracker.gradient(f, x)[1]\n",
    "@show df(2) \n",
    "\n",
    "# d²f/dx² = 6\n",
    "d2f(x) = Tracker.gradient(df, x)[1]\n",
    "@show d2f(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（これらの数字が`tracked`の後に表示される理由については後で学ぶ。）\n",
    "\n",
    "関数が沢山パラメータを保つ場合、それらをすべて明示的に渡せる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0 (tracked), 1.0, 2.0 (tracked))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(W, b, x) = W * x + b\n",
    "\n",
    "Tracker.gradient(f, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しかし機械学習モデルは数百のパラメータを持つことができる。  \n",
    "Fluxはこれを処理するためのいい方法を提供する。Fluxに何らかのパラメータをparamで扱うことを指示できる。  \n",
    "その結果、これらを一緒にまとめる事ができ、`gradient`にすべての勾配を一度に集めるよう指示できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grads[W] = 4.0\n",
      "grads[b] = 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = param(2) # 2.0 (tracked)\n",
    "b = param(3) # 3.0 (tracked)\n",
    "\n",
    "f(x) = W * x + b\n",
    "\n",
    "params = Params([W, b])\n",
    "grads  = Tracker.gradient( ()-> f(4), params)\n",
    "\n",
    "@show grads[W] # 4.0\n",
    "@show grads[b] # 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで注意点が少しある。まず、今`tracked`として表示されている`W`と`b`。  \n",
    "追跡されたものは普通の数字や配列のように振る舞うが、それらを使って君がやったことをすべて記録して、Fluxはそれらの勾配を計算する事ができる。  \n",
    "`gradient`は引数なしの関数を受け取る。Paramsが（引数の）識別を教えてくれるため、引数は必要ない。\n",
    "\n",
    "これは、巨大で複雑なモデルを扱うときに本当に役立つだろう。だが今の所は、簡単なものから始めよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Models\n",
    "\n",
    "入力`x`から出力配列`y`の予測を試みるシンプルな線形回帰を考える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.226045212625105"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = rand(2, 5)\n",
    "b = rand(2)\n",
    "\n",
    "predict(x) = W*x .+ b\n",
    "\n",
    "function loss(x, y)\n",
    "    ŷ = predict(x)\n",
    "    sum((y .- ŷ).^2)\n",
    "end\n",
    "\n",
    "x, y = rand(5), rand(2) # ダミーのデータ\n",
    "loss(x,y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "予測を改良するために、lossに対して`W`と`b`の勾配を取って勾配降下を行う。上記と同じようにして`W`と`b`がパラメータであるとFluxに伝えよう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grads(...)\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux.Tracker\n",
    "\n",
    "W = param(W)\n",
    "b = param(b)\n",
    "\n",
    "gs = Tracker.gradient( \n",
    "    ()-> loss(x,y),\n",
    "    Params([W, b])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配を得たので、それらを取り出して`W`を更新してモデルを更新できる。`update!(W, Δ)`関数は`W = W + Δ`を適用でき、勾配降下法のために利用できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6614529368498637 (tracked)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux.Tracker: update!\n",
    "\n",
    "Δ = gs[W]\n",
    "\n",
    "# パラメータを更新して勾配をリセットする\n",
    "update!(W, -0.1Δ)\n",
    "\n",
    "loss(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lossの値は少し減少しており、これは予測値xが目標のyに近づいていることを意味している。もしデータが有るなら、既にモデルのトレーニングを試すことができる。\n",
    "\n",
    "Fluxでのすべての深層学習は、複雑ではあるが、この例を単に一般化したものだ。もちろん、モデルはとても難しく見えるかもしれないし、数百万のパラメータや複雑な制御フローを持っているかもしれない。Fluxがもっと複雑なモデルをどう扱うのか見てみよう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Layers\n",
    "\n",
    "上記の線形回帰よりも複雑なモデルを作成するのが一般的だ。例えば、２つの線形レイヤとその間に`sigmoid`(σ)のような非線形性を持つレイヤーを作る事ができる。上記ならこれを次のようにかける。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tracked 2-element Array{Float64,1}:\n",
       " 0.7222072449261435\n",
       " 2.4120757798432053"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "\n",
    "W1 = param(rand(3,5))\n",
    "b1 = param(rand(3))\n",
    "layer1(x) = W1 * x .+ b1\n",
    "\n",
    "W2 = param(rand(2,3))\n",
    "b2 = param(rand(2))\n",
    "layer2(x) = W2 * x .+ b2\n",
    "\n",
    "model(x) = layer2(σ.(layer1(x)))\n",
    "\n",
    "model(rand(5)) # => 2要素のVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これはうまくいくが扱いづらい、特にもっとレイヤを追加すると繰り返しが多くなる。これをなくす一つの方法は線形レイヤを返す関数を作ることだ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tracked 2-element Array{Float64,1}:\n",
       " -0.20036533257623934\n",
       "  0.25152498388618283"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function linear(in, out)\n",
    "    W = param(randn(out, in))\n",
    "    b = param(randn(out))\n",
    "    x -> W * x .+ b\n",
    "end\n",
    "\n",
    "linear1 = linear(5,3)\n",
    "linear2 = linear(3,2)\n",
    "\n",
    "model(x) = linear2(σ.(linear1(x)))\n",
    "\n",
    "model(rand(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "他の（同様の）方法は、アフィンレイヤーを明示的に示す構造体を作ることだ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tracked 5-element Array{Float64,1}:\n",
       " -3.6168585000919258 \n",
       "  0.4222918259118986 \n",
       " -1.2630088382877553 \n",
       " -1.0990569127164882 \n",
       "  0.39323382543713437"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Affine\n",
    "    W\n",
    "    b\n",
    "end\n",
    "\n",
    "Affine(in::Integer, out::Integer) = Affine(param(randn(out, in)), param(randn(out)))\n",
    "\n",
    "# オブジェクトを関数として呼び出しているからオーバーロード呼び出し\n",
    "(m::Affine)(x) = m.W * x .+ m.b\n",
    "\n",
    "a = Affine(10,5)\n",
    "\n",
    "a(rand(10)) # 5要素のVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "おめでとう！ 君はFluxの`Dense`レイヤを作ったぞ。Fluxは沢山の面白いレイヤを持っているが、それらは君自身が簡単に作れるものだ。\n",
    "\n",
    "(`Dense`との小さな違いはある。- 便宜上、活性化関数も必要なのだ、`Dense(10, 5, σ)`のような。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking It Up\n",
    "\n",
    "次のようなモデルを書くのが普通だ。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "layer1 = Dense(10,5,σ)\n",
    "\n",
    "# ...\n",
    "\n",
    "model(x) = layer3(layer2(layer1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "長いチェーンの場合、次のようにレイヤのリストをもつ方がちょっと一般的かも。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tracked 2-element Array{Float64,1}:\n",
       " 0.22502193496667067\n",
       " 0.7749780650333293 "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Flux\n",
    "\n",
    "layers = [Dense(10,5,σ), Dense(5,2), softmax]\n",
    "\n",
    "model(x) = foldl((x,m)-> m(x), layers, init=x)\n",
    "\n",
    "model(rand(10)) # => 2要素のVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "便利なことに、これはFluxでも提供されている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tracked 2-element Array{Float64,1}:\n",
       " 0.3447999270770912\n",
       " 0.6552000729229088"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Chain(\n",
    "    Dense(10,5,σ),\n",
    "    Dense(5,2),\n",
    "    softmax)\n",
    "\n",
    "model2(rand(10)) # => 2要素のVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これはすぐハイレベルな深層学習ライブラリのように見える。その上さらに、シンプルな抽象化から抜け出たと見れ、そして、Juliaコードのパワーを失わない。\n",
    "\n",
    "このアプローの優れた特性は、\"models\"は単なる関数（トレーニングすることができるパラメータを含む）なので、これをシンプルな関数の組み合わせだと見なせることだ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tracked 2-element Array{Float64,1}:\n",
       " -0.5736828240665447\n",
       " -0.7610193321469176"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Dense(5,2) ∘ Dense(10, 5, σ) # \\circ\n",
    "\n",
    "m(rand(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同様に、`Chain`はJuliaのどの関数でもうまく動くだろう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Chain(x->x^2, x->x+1)\n",
    "m(5) # => 26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer helpers\n",
    "\n",
    "Fluxはカスタムレイヤのためのヘルパーのセットを提供する。次のように呼び出すと可能になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@treelike Affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これでパラメータの収集やGPUへの移動などの`Affine`レイヤーへの便利な追加機能セットが使えるようになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.2",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
